#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ê°œì„ ëœ CSV ì§ì ‘ ê²€ìƒ‰ ì‹œìŠ¤í…œ
- ì„±ëŠ¥ ìµœì í™”
- ì •í™•í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
- ë¹ ë¥¸ ê²€ìƒ‰ ì†ë„
"""

import pandas as pd
import re
from typing import List, Dict, Any
import logging
import time

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ImprovedCSVDirectSearch:
    def __init__(self, csv_file_path: str):
        """ê°œì„ ëœ CSV ì§ì ‘ ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.csv_file_path = csv_file_path
        self.df = None
        self.search_cache = {}  # ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ
        self.load_csv_data()
    
    def load_csv_data(self):
        """CSV íŒŒì¼ ë¡œë“œ ë° ìµœì í™”"""
        try:
            logger.info(f"CSV íŒŒì¼ ë¡œë”© ì¤‘: {self.csv_file_path}")
            self.df = pd.read_csv(self.csv_file_path, encoding='utf-8')
            
            # ë°ì´í„° ì „ì²˜ë¦¬
            self.df = self.df.fillna("")
            
            # ì¤‘ë³µ ì œê±° (í—ˆê°€ë²ˆí˜¸ ê¸°ì¤€)
            self.df = self.df.drop_duplicates(subset=['í—ˆê°€ë²ˆí˜¸'], keep='first')
            
            # ê²€ìƒ‰ìš© ì»¬ëŸ¼ ìƒì„± (ë¹ ë¥¸ ê²€ìƒ‰ì„ ìœ„í•´)
            self.df['search_text'] = (
                self.df['ì œí’ˆëª…'].astype(str) + " " +
                self.df['ì£¼ì„±ë¶„'].astype(str) + " " +
                self.df['ì—…ì²´ëª…'].astype(str) + " " +
                self.df['ì œí’ˆì˜ë¬¸ëª…'].astype(str) + " " +
                self.df['ì£¼ì„±ë¶„ì˜ë¬¸'].astype(str)
            ).str.lower()
            
            logger.info(f"ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(self.df)} í–‰ (ì¤‘ë³µ ì œê±° í›„)")
            
        except Exception as e:
            logger.error(f"CSV íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    def extract_keywords(self, query: str) -> List[str]:
        """ê°œì„ ëœ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []
        
        # 1. ì˜ì•½í’ˆ ê´€ë ¨ íŒ¨í„´
        medicine_patterns = [
            r'[ê°€-í£]+ì •',  # ~ì •
            r'[ê°€-í£]+ì•¡',  # ~ì•¡
            r'[ê°€-í£]+ìº¡ìŠ',  # ~ìº¡ìŠ
            r'[ê°€-í£]+ì‹œëŸ½',  # ~ì‹œëŸ½
            r'[ê°€-í£]+ì£¼',  # ~ì£¼
            r'[ê°€-í£]+í¬ë¦¼',  # ~í¬ë¦¼
            r'[ê°€-í£]+ì—°ê³ ',  # ~ì—°ê³ 
            r'[ê°€-í£]+ì œ',  # ~ì œ
            r'[ê°€-í£]+ì•½',  # ~ì•½
            r'[ê°€-í£]+ë§ˆìŠ¤í¬',  # ~ë§ˆìŠ¤í¬
            r'[ê°€-í£]+ì†Œë…',  # ~ì†Œë…
            r'[ê°€-í£]+ì¹˜ë£Œì œ',  # ~ì¹˜ë£Œì œ
            r'[ê°€-í£]+í•­ìƒì œ',  # ~í•­ìƒì œ
            r'[ê°€-í£]+ì†Œí™”ì œ',  # ~ì†Œí™”ì œ
            r'[ê°€-í£]+ì§„í†µì œ',  # ~ì§„í†µì œ
            r'[ê°€-í£]+í˜ˆì••ì•½',  # ~í˜ˆì••ì•½
            r'[ê°€-í£]+ë‹¹ë‡¨ì•½',  # ~ë‹¹ë‡¨ì•½
        ]
        
        # íŒ¨í„´ ë§¤ì¹­
        for pattern in medicine_patterns:
            matches = re.findall(pattern, query)
            keywords.extend(matches)
        
        # 2. ì¼ë°˜ ì˜ì•½í’ˆëª… (3ê¸€ì ì´ìƒ)
        medicine_names = re.findall(r'[ê°€-í£]{3,}', query)
        keywords.extend(medicine_names)
        
        # 3. ì˜ì–´ ì˜ì•½í’ˆëª… (3ê¸€ì ì´ìƒ)
        english_names = re.findall(r'[a-zA-Z]{3,}', query)
        keywords.extend(english_names)
        
        # 4. ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±°
        stop_words = {
            'ì—', 'ì˜', 'ë¥¼', 'ì„', 'ê°€', 'ì´', 'ëŠ”', 'ì€', 'ë„', 'ë§Œ', 'ëŒ€í•œ', 
            'ì •ë³´ë¥¼', 'ì•Œë ¤ì£¼ì„¸ìš”', 'ì–´ë–¤', 'ê²ƒë“¤ì´', 'ìˆë‚˜ìš”', 'ëŒ€í•´ì„œ', 'ì•Œë ¤ì£¼',
            'ë­”ê°€ìš”', 'ë¬´ì—‡ì¸ê°€ìš”', 'ì•Œë ¤ì£¼ì„¸ìš”', 'ì•Œë ¤ì¤˜', 'ì •ë³´', 'ëŒ€í•´ì„œ',
            'ë­ì•¼', 'ë¬´ì—‡', 'ì–´ë–¤', 'ê²ƒë“¤', 'ì¤‘ì—ì„œ', 'ìˆë‚˜ìš”', 'ì•Œë ¤ì¤˜',
            'ì£¼ì„±ë¶„', 'ì„±ë¶„', 'ì œí’ˆ', 'ì•½í’ˆ', 'ì˜ì•½í’ˆ', 'ì œí˜•', 'ì—…ì²´', 'íšŒì‚¬'
        }
        
        # ì¡°ì‚¬ ì œê±° ë° í•„í„°ë§
        cleaned_keywords = []
        for keyword in keywords:
            # ì¡°ì‚¬ ì œê±°
            for stop_word in stop_words:
                if keyword.endswith(stop_word):
                    keyword = keyword[:-len(stop_word)]
                    break
            
            # 3ê¸€ì ì´ìƒì´ê³  stop_wordsì— ì—†ëŠ” ê²½ìš°ë§Œ ì¶”ê°€
            if len(keyword) >= 3 and keyword not in stop_words:
                cleaned_keywords.append(keyword)
        
        # ì¤‘ë³µ ì œê±°
        cleaned_keywords = list(set(cleaned_keywords))
        
        logger.info(f"ì¶”ì¶œëœ í‚¤ì›Œë“œ: {cleaned_keywords}")
        return cleaned_keywords
    
    def fast_search(self, query: str) -> Dict[str, Any]:
        """ë¹ ë¥¸ ê²€ìƒ‰ ìˆ˜í–‰"""
        start_time = time.time()
        
        try:
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self.extract_keywords(query)
            
            if not keywords:
                return {
                    "keywords": [],
                    "total_count": 0,
                    "report": "í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "data": [],
                    "search_time": 0
                }
            
            # ìºì‹œ í™•ì¸
            cache_key = " ".join(sorted(keywords))
            if cache_key in self.search_cache:
                result = self.search_cache[cache_key].copy()
                result["search_time"] = time.time() - start_time
                return result
            
            # ë¹ ë¥¸ ê²€ìƒ‰ (search_text ì»¬ëŸ¼ ì‚¬ìš©)
            search_results = []
            for keyword in keywords:
                mask = self.df['search_text'].str.contains(keyword, case=False, na=False)
                keyword_results = self.df[mask].to_dict('records')
                search_results.extend(keyword_results)
            
            # ì¤‘ë³µ ì œê±° (í—ˆê°€ë²ˆí˜¸ ê¸°ì¤€)
            unique_results = []
            seen_licenses = set()
            for result in search_results:
                license_num = result.get('í—ˆê°€ë²ˆí˜¸', '')
                if license_num not in seen_licenses:
                    seen_licenses.add(license_num)
                    unique_results.append(result)
            
            total_count = len(unique_results)
            
            if total_count == 0:
                result = {
                    "keywords": keywords,
                    "total_count": 0,
                    "report": f"'{', '.join(keywords)}'ì™€ ê´€ë ¨ëœ ì˜ì•½í’ˆ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "data": []
                }
            else:
                # ë³´ê³ ì„œ ìƒì„±
                report = self.generate_improved_report(unique_results, keywords)
                result = {
                    "keywords": keywords,
                    "total_count": total_count,
                    "report": report,
                    "data": unique_results
                }
            
            # ìºì‹œì— ì €ì¥
            self.search_cache[cache_key] = result.copy()
            result["search_time"] = time.time() - start_time
            
            return result
            
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return {
                "keywords": [],
                "total_count": 0,
                "report": f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}",
                "data": [],
                "search_time": time.time() - start_time
            }
    
    def generate_improved_report(self, data: List[Dict], keywords: List[str]) -> str:
        """ê°œì„ ëœ ë³´ê³ ì„œ ìƒì„±"""
        if not data:
            return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        report_lines = []
        report_lines.append(f"=== ì˜ì•½í’ˆ ê²€ìƒ‰ ë³´ê³ ì„œ ===")
        report_lines.append(f"ê²€ìƒ‰ í‚¤ì›Œë“œ: {', '.join(keywords)}")
        report_lines.append(f"ì´ ê²€ìƒ‰ ê²°ê³¼: {len(data)}ê°œ")
        report_lines.append("")
        
        # 1. ì œí’ˆ ë¶„ë¥˜ë³„ í†µê³„
        classifications = {}
        for item in data:
            classification = item.get('í’ˆëª©ë¶„ë¥˜', 'ë¯¸ë¶„ë¥˜')
            if classification and classification != 'ë¯¸ë¶„ë¥˜':
                if classification not in classifications:
                    classifications[classification] = 0
                classifications[classification] += 1
        
        if classifications:
            report_lines.append("ğŸ“Š ì œí’ˆ ë¶„ë¥˜ë³„ í†µê³„:")
            for classification, count in sorted(classifications.items(), key=lambda x: x[1], reverse=True):
                report_lines.append(f"  â€¢ {classification}: {count}ê°œ")
            report_lines.append("")
        
        # 2. ì—…ì²´ë³„ í†µê³„
        companies = {}
        for item in data:
            company = item.get('ì—…ì²´ëª…', 'ë¯¸ìƒ')
            if company and company != 'ë¯¸ìƒ':
                if company not in companies:
                    companies[company] = 0
                companies[company] += 1
        
        if companies:
            report_lines.append("ğŸ¢ ì—…ì²´ë³„ í†µê³„:")
            for company, count in sorted(companies.items(), key=lambda x: x[1], reverse=True)[:10]:
                report_lines.append(f"  â€¢ {company}: {count}ê°œ")
            report_lines.append("")
        
        # 3. ì œí˜•ë³„ í†µê³„
        forms = {}
        for item in data:
            form = item.get('ì œí˜•', 'ë¯¸ìƒ')
            if form and form != 'ë¯¸ìƒ':
                if form not in forms:
                    forms[form] = 0
                forms[form] += 1
        
        if forms:
            report_lines.append("ğŸ’Š ì œí˜•ë³„ í†µê³„:")
            for form, count in sorted(forms.items(), key=lambda x: x[1], reverse=True):
                report_lines.append(f"  â€¢ {form}: {count}ê°œ")
            report_lines.append("")
        
        # 4. ìƒì„¸ ì œí’ˆ ì •ë³´ (ìµœëŒ€ 15ê°œ)
        report_lines.append("ğŸ“‹ ìƒì„¸ ì œí’ˆ ì •ë³´:")
        display_count = min(15, len(data))
        
        for i, item in enumerate(data[:display_count], 1):
            report_lines.append(f"\n{i}. {item.get('ì œí’ˆëª…', 'ì œí’ˆëª… ì—†ìŒ')}")
            report_lines.append(f"   â€¢ ì—…ì²´: {item.get('ì—…ì²´ëª…', 'ë¯¸ìƒ')}")
            
            main_ingredient = item.get('ì£¼ì„±ë¶„', '')
            if main_ingredient:
                report_lines.append(f"   â€¢ ì£¼ì„±ë¶„: {main_ingredient}")
            
            form = item.get('ì œí˜•', '')
            if form:
                report_lines.append(f"   â€¢ ì œí˜•: {form}")
            
            report_lines.append(f"   â€¢ í—ˆê°€ë²ˆí˜¸: {item.get('í—ˆê°€ë²ˆí˜¸', 'ë¯¸ìƒ')}")
            report_lines.append(f"   â€¢ ì „ë¬¸/ì¼ë°˜: {item.get('ì „ë¬¸ì˜ì•½í’ˆ', 'ë¯¸ìƒ')}")
            
            # ì¶”ê°€ ì •ë³´
            atc_code = item.get('ATCì½”ë“œ', '')
            if atc_code:
                report_lines.append(f"   â€¢ ATCì½”ë“œ: {atc_code}")
            
            english_name = item.get('ì œí’ˆì˜ë¬¸ëª…', '')
            if english_name:
                report_lines.append(f"   â€¢ ì˜ë¬¸ëª…: {english_name}")
        
        if len(data) > display_count:
            report_lines.append(f"\n... ì™¸ {len(data) - display_count}ê°œ ì œí’ˆ")
        
        return "\n".join(report_lines)

def query_improved_csv_search(query: str, csv_search: ImprovedCSVDirectSearch):
    """ê°œì„ ëœ CSV ê²€ìƒ‰ ì‹œìŠ¤í…œì— ì§ˆë¬¸"""
    result = csv_search.fast_search(query)
    return result

if __name__ == "__main__":
    # CSV íŒŒì¼ ê²½ë¡œ ì„¤ì •
    csv_file_path = "C:/Users/tel33/OneDrive/ë°”íƒ• í™”ë©´/2025ë…„ í•˜ê³„ë°©í•™/7ì›” ì¸í„´ ë„¥ìŠ¤ì œë„ˆ/codes/ê°„ë‹¨í•œ DB./Medical_product_data.csv"
    
    try:
        # ê°œì„ ëœ CSV ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        csv_search = ImprovedCSVDirectSearch(csv_file_path)
        
        print("=== ê°œì„ ëœ CSV ì§ì ‘ ê²€ìƒ‰ ì˜ì•½í’ˆ ì‹œìŠ¤í…œ ===")
        print("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ì…ë ¥)")
        print("ì˜ˆì‹œ ì§ˆë¬¸:")
        print("- ì•„ìŠ¤í”¼ë¦°ì— ëŒ€í•´ì„œ ì•Œë ¤ì£¼ì„¸ìš”")
        print("- íƒ€ì´ë ˆë†€ì˜ ì£¼ì„±ë¶„ì´ ë­ì•¼?")
        print("- ë³´ê±´ìš©ë§ˆìŠ¤í¬ ì¤‘ì—ì„œ ì–´ë–¤ ê²ƒë“¤ì´ ìˆë‚˜ìš”?")
        
        while True:
            query = input("\nì§ˆë¬¸: ").strip()
            
            if query.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                print("ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            if not query:
                print("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
            
            try:
                print("ê²€ìƒ‰ ì¤‘...")
                result = query_improved_csv_search(query, csv_search)
                
                print(f"\nì¶”ì¶œëœ í‚¤ì›Œë“œ: {result['keywords']}")
                print(f"ì°¸ê³  ë°ì´í„° ìˆ˜: {result['total_count']}")
                print(f"ê²€ìƒ‰ ì‹œê°„: {result.get('search_time', 0)*1000:.2f}ms")
                print(f"\n{result['report']}")
                
            except Exception as e:
                print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    except Exception as e:
        print(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}") 